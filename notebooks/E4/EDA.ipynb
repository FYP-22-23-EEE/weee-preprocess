{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "# find the dataset directory\n",
    "while ! [ -d \"data/v1\" ]; do\n",
    "    if [ \"$(pwd)\" == \"/\" ]; then    \n",
    "        echo \"not found.\"\n",
    "        exit 1\n",
    "    fi\n",
    "    cd ..\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install neurokit2 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neurokit2 as nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"data/v1\"\n",
    "SAMPLING_RATE = 4 # Hz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudyInfoEncoder:\n",
    "    \n",
    "    def __init__(self, dataset_dir=DATASET_DIR, filename=\"Study_Information.csv\"):\n",
    "        self.activities = ['Start_Sit', 'Start_Stand', 'Start_Cycle1', 'Start_Cycle2', 'Start_Run1', 'Start_Run2']\n",
    "        self.encodings = { v:i for i, v in  enumerate(self.activities)}\n",
    "        self.info = pd.read_csv(\n",
    "            os.path.join(DATASET_DIR, filename),\n",
    "            parse_dates=list(self.encodings.keys())\n",
    "        )\n",
    "    \n",
    "    def fit_activity_column(self, df, participant_id, timstamp_column, activity_column='activity', activity_column_index=1):\n",
    "        _info = self.info[self.info['Participant'] == f\"P{participant_id:02d}\"]\n",
    "        df.insert(activity_column_index, activity_column, np.nan)\n",
    "        df[activity_column] = df[activity_column].astype('Int8')\n",
    "        for activity in self.activities:\n",
    "            df.loc[df[timstamp_column] >= _info[activity].iloc[0], activity_column] = self.encodings[activity]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_dir, patient, sampling_rate):\n",
    "    \"\"\"\n",
    "        Load EDA data from the E4 dataset.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(os.path.join(dataset_dir, f\"P{patient:02d}\", \"E4\", \"EDA.csv\"), header=None)\n",
    "    start_time = datetime.datetime.fromtimestamp(float(data.iloc[0, 0]), tz=datetime.timezone.utc)\n",
    "    start_time = start_time.replace(tzinfo=None)\n",
    "    time_gap = int(1000/sampling_rate) # ms\n",
    "    data = pd.DataFrame({\n",
    "        'timestamp': pd.date_range(start=start_time, periods=len(data.iloc[1:]), freq=f\"{time_gap}ms\"),\n",
    "        'eda_raw': data.iloc[1:].values.reshape(-1),\n",
    "    })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_eda(eda_data, upsample_rate, timestamp_col='timestamp', eda_col='eda_raw'):\n",
    "    \"\"\"\n",
    "        Upsample EDA data.\n",
    "    :param eda_data: EDA dataframe with timestamp and eda_raw columns\n",
    "    :param upsample_rate: Upsample rate in Hz\n",
    "    \"\"\"\n",
    "    data = eda_data.set_index(timestamp_col).resample(f\"{1000/upsample_rate}ms\").mean().reset_index()\n",
    "    data[eda_col] = data[eda_col].interpolate(method='linear')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change coloumn names\n",
    "import re\n",
    "def to_snake_case(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    s2 = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "    return re.sub('_+', '_', s2)\n",
    "\n",
    "def standardize_column_names(df):\n",
    "    df = df.rename(\n",
    "        columns={old: to_snake_case(old) for old in df.columns},\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/v1/Study_Information.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sie \u001b[39m=\u001b[39m StudyInfoEncoder()\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_task\u001b[39m(p):\n\u001b[1;32m      4\u001b[0m     \u001b[39mglobal\u001b[39;00m sie\n",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mStudyInfoEncoder.__init__\u001b[0;34m(self, dataset_dir, filename)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivities \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mStart_Sit\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mStart_Stand\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mStart_Cycle1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mStart_Cycle2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mStart_Run1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mStart_Run2\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencodings \u001b[39m=\u001b[39m { v:i \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m  \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivities)}\n\u001b[0;32m----> 6\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\n\u001b[1;32m      7\u001b[0m     os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(DATASET_DIR, filename),\n\u001b[1;32m      8\u001b[0m     parse_dates\u001b[39m=\u001b[39;49m\u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencodings\u001b[39m.\u001b[39;49mkeys())\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/weee-preprocess-I3n8R8lm/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/weee-preprocess-I3n8R8lm/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/weee-preprocess-I3n8R8lm/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/weee-preprocess-I3n8R8lm/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/weee-preprocess-I3n8R8lm/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/weee-preprocess-I3n8R8lm/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/weee-preprocess-I3n8R8lm/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/v1/Study_Information.csv'"
     ]
    }
   ],
   "source": [
    "sie = StudyInfoEncoder()\n",
    "\n",
    "def process_task(p):\n",
    "    global sie\n",
    "    df = load_dataset(DATASET_DIR, p, SAMPLING_RATE)\n",
    "    \n",
    "    # process eda\n",
    "    df = upsample_eda(df, upsample_rate=SAMPLING_RATE*2)\n",
    "    signals, info = nk.eda_process(df['eda_raw'], sampling_rate=SAMPLING_RATE*2)\n",
    "    df = pd.concat([df['timestamp'], signals], axis=1)\n",
    "    \n",
    "    # post touch ups\n",
    "    df.insert(1, 'participant', p)\n",
    "    df.insert(2, 'activity', np.nan)\n",
    "    df = sie.fit_activity_column(df, p, 'timestamp')\n",
    "    df = standardize_column_names(df)\n",
    "    print(f\"Done processing participant {p:02d}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing participant 01\n",
      "Done processing participant 02\n",
      "Done processing participant 03\n",
      "Done processing participant 04\n",
      "Done processing participant 05\n",
      "Done processing participant 06\n",
      "Done processing participant 07\n",
      "Done processing participant 08\n",
      "Done processing participant 09\n",
      "Done processing participant 10\n",
      "Done processing participant 11\n",
      "Done processing participant 12\n",
      "Done processing participant 13\n",
      "Done processing participant 14\n",
      "Done processing participant 15\n",
      "Done processing participant 16\n",
      "Done processing participant 17\n"
     ]
    }
   ],
   "source": [
    "# concat all participants row wise\n",
    "E4_EDA = pd.concat([process_task(p) for p in range(1, 18)], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp                 0\n",
       "participant               0\n",
       "activity             145760\n",
       "eda_raw                   0\n",
       "eda_clean                 0\n",
       "eda_tonic                 0\n",
       "eda_phasic                0\n",
       "scr_onsets                0\n",
       "scr_peaks                 0\n",
       "scr_height                0\n",
       "scr_amplitude             0\n",
       "scr_rise_time             0\n",
       "scr_recovery              0\n",
       "scr_recovery_time       171\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nan count per column\n",
    "E4_EDA.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZES = ['2s', '4s', '6s', '8s', '10s', '12s']\n",
    "AGG_FUNCS = ['mean', 'std', 'min', 'max', 'median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_non_overlapping_windows(window_size, agg_funcs, ignore_incomplete_windows=True):\n",
    "    dfs = []\n",
    "    for p, p_df in E4_EDA.groupby('participant'):\n",
    "        for a, pa_df in p_df.groupby('activity'):\n",
    "            # pre touch ups\n",
    "            df = pa_df.sort_values('timestamp')\n",
    "            df = df.dropna(subset=['activity']) # remove rows with nan activity\n",
    "            if ignore_incomplete_windows:\n",
    "                number_of_windows = len(df) // int(window_size[:-1])\n",
    "                df = df.iloc[:number_of_windows*int(window_size[:-1])] # remove incomplete windows\n",
    "            \n",
    "            # create windows and aggregate\n",
    "            agg_cols = {c: agg_funcs for c in df.columns if c not in ['timestamp', 'activity', 'participant']}\n",
    "            agg_cols['timestamp'] =  ['first', 'last']\n",
    "            windowed = df.groupby(pd.Grouper(key='timestamp', freq=window_size)).agg(agg_cols)\n",
    "\n",
    "            # post touch ups\n",
    "            windowed.columns = ['_'.join(col).strip() for col in windowed.columns.values]\n",
    "            windowed = windowed.reset_index()\n",
    "            windowed.insert(0, 'participant', p)\n",
    "            windowed.insert(1, 'activity', a)\n",
    "            windowed.insert(2, 'window', range(len(windowed)))\n",
    "            dfs.append(windowed)\n",
    "    return pd.concat(dfs, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir -p data/v2\n",
    "DATASET_DIR_V2 = \"data/v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing window size 2s\n",
      "Done processing window size 4s\n",
      "Done processing window size 6s\n",
      "Done processing window size 8s\n",
      "Done processing window size 10s\n",
      "Done processing window size 12s\n"
     ]
    }
   ],
   "source": [
    "for ws in WINDOW_SIZES:\n",
    "    E4_EDA_WINDOWED = create_non_overlapping_windows(ws, AGG_FUNCS)\n",
    "    E4_EDA_WINDOWED.to_csv(os.path.join(DATASET_DIR_V2, 'E4', f\"E4_EDA_{ws}.csv\"), index=False)\n",
    "    print(f\"Done processing window size {ws}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weee-preprocess-I3n8R8lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "272a8a6e35ded4b5bc2149586ac0a4b36927350a5bf7c6991efb05b52c5edcf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
